{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 5)\t2\n",
      "  (0, 9)\t1\n",
      "  (1, 2)\t2\n",
      "  (1, 8)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 3)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "s1 = \"This is a TV and it is a washing machine\"\n",
    "s2 = \"Hi how are you my friend friend\"\n",
    "l=[s1,s2]\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "bow=count_vect.fit_transform(l)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "\n",
      "Word Tokens:  ['Text', 'messaging', ',', 'or', 'texting', ',', 'is', 'the', 'act', 'of', 'composing', 'and', 'sending', 'electronic', 'messages', ',', 'typically', 'consisting', 'of', 'alphabetic', 'and', 'numeric', 'characters', ',', 'between', 'two', 'or', 'more', 'users', 'of', 'mobile', 'devices', ',', 'desktops/laptops', ',', 'or', 'other', 'type', 'of', 'compatible', 'computer', '.', 'Text', 'messages', 'may', 'be', 'sent', 'over', 'a', 'cellular', 'network', ',', 'or', 'may', 'also', 'be', 'sent', 'via', 'an', 'Internet', 'connection', '.']\n",
      "\n",
      "Filtered Bag of Words after removing stop words are :\n",
      " ['Text', 'messaging', ',', 'texting', ',', 'act', 'composing', 'sending', 'electronic', 'messages', ',', 'typically', 'consisting', 'alphabetic', 'numeric', 'characters', ',', 'two', 'users', 'mobile', 'devices', ',', 'desktops/laptops', ',', 'type', 'compatible', 'computer', '.', 'Text', 'messages', 'may', 'sent', 'cellular', 'network', ',', 'may', 'also', 'sent', 'via', 'Internet', 'connection', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "s=\"Text messaging, or texting, is the act of composing and sending electronic messages, typically consisting of alphabetic and numeric characters, between two or more users of mobile devices, desktops/laptops, or other type of compatible computer. Text messages may be sent over a cellular network, or may also be sent via an Internet connection.\"\n",
    "print(\"Stop words: \\n\",stopwords.words('english'))\n",
    "\n",
    "word_token = word_tokenize(s)\n",
    "print(\"\\n\\nWord Tokens: \",word_token)\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "filtered_words = [w for w in word_token if w not in stop]\n",
    "print(\"\\nFiltered Bag of Words after removing stop words are :\\n\",filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "### PorterStemmer and LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words               PorterStemmer       LancasterStemmer    \n",
      "firend              firend              firend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "destablize          destabl             dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "word_list=['firend','friendship','friends','friendships','destablize','misunderstanding','railroad','moonlight','football']\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print('{0:20}{1:20}{2:20}'.format('words','PorterStemmer','LancasterStemmer'))\n",
    "for w in word_list:\n",
    "    print('{0:20}{1:20}{2:20}'.format(w,ps.stem(w),ls.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbs = SnowballStemmer('english')\n",
    "sbs.stem('having')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('sachin.txt',mode='r',encoding='utf-8')\n",
    "mylines = file.readlines()\n",
    "\n",
    "def PorterStemming(lines):\n",
    "    ps = PorterStemmer()\n",
    "    word_token = word_tokenize(lines)\n",
    "    stem_sentence=[]\n",
    "    \n",
    "    for w in word_token:\n",
    "        stem_sentence.append(ps.stem(w))\n",
    "        stem_sentence.append(\" \")\n",
    "    \n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "def LancasterStemming(lines):\n",
    "    ps = LancasterStemmer()\n",
    "    word_token = word_tokenize(lines)\n",
    "    stem_sentence=[]\n",
    "    \n",
    "    for w in word_token:\n",
    "        stem_sentence.append(ps.stem(w))\n",
    "        stem_sentence.append(\" \")\n",
    "    \n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "f1 = open('Porter_Sachin.txt','w')\n",
    "f2 = open('Lancaster_Sachin.txt','w')\n",
    "\n",
    "for l in mylines:\n",
    "    s1 = PorterStemming(l)\n",
    "    s2 = LancasterStemming(l)\n",
    "    f1.writelines(s1)\n",
    "    f2.writelines(s2)\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PorterStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c519e8f17b52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sachin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sachin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'PorterStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "print(PorterStemmer().stem(\"Sachin\"),LancasterStemmer().stem(\"Sachin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
